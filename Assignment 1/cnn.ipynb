{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef0a2612-e543-4fa9-a327-7d072ca353fc",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN)\n",
    "### Introduction\n",
    "This code details the implementation of the LeNet-5 convolutional neural network (CNN) architecture to classify images from the MNIST dataset. The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9). The goal is to train a model that can accurately classify these images into one of the 10 digit classes.\n",
    "\n",
    "### Implementation\n",
    "By using PyTorch library, LeNet-5 can be modelled and trained. The following hyperparameters are required to define:\n",
    "- `batch_size`: Number of samples per batch.\n",
    "- `num_classes`: Number of output classes (10 for digits 0-9).\n",
    "- `learning_rate`: Learning rate for the optimizer.\n",
    "- `num_epochs`: Number of times the entire dataset is passed through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f3fdb2-e1e0-4447-a746-ebff592d7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in relevant libraries, and alias where appropriate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define relevant variables for the ML task\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d17a73-0aa5-4fbb-959d-ccae8287f1a5",
   "metadata": {},
   "source": [
    "### Data Loading and Transformation\n",
    "We load the MNIST dataset and apply transformations to resize the images to 32x32 (as required by LeNet-5), convert them to tensors, and normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed8ff56-3c0d-413d-8645-195cc2235527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset and preprocessing\n",
    "train_dataset = datasets.MNIST(root = './MNIST',\n",
    "                               train = True,\n",
    "                               transform = transforms.Compose([\n",
    "                               transforms.Resize((32,32)),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                               download = True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root = './MNIST',\n",
    "                              train = False,\n",
    "                              transform = transforms.Compose([\n",
    "                              transforms.Resize((32,32)),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                              download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6bd18a-769d-41d4-b428-6b31cc3d9506",
   "metadata": {},
   "source": [
    "### Apply the CNN (Le-Net5) model\n",
    "We define the LeNet-5 model, which consists of two convolutional layers followed by three fully connected layers. The activation function used is `Tanh`, and `MaxPool2d` is used for pooling. The final layer uses `Softmax` for classification.\\\n",
    "\\\n",
    "Note:\n",
    "- Include 3 parts: convolution, pooling, non-linear activation function\n",
    "- LeNet-5 CNN architecture has 7 layers, 3 convolution, 2 subsampling and 2 fully linked layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7d30c-97f3-4eed-9ea6-e9f236939a88",
   "metadata": {},
   "source": [
    "![LeNet-5](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*fDvp2DDqNMPEUkmp6kijJw.jpeg)\n",
    "\n",
    "#### Original Model:\n",
    "<img src=https://miro.medium.com/v2/resize:fit:1100/format:webp/1*zxrGm9YBq__CPE3EUZKDOQ.jpeg width=\"700\">\n",
    "\n",
    "After some modifications towards the model, I have improved it's accuracy from **~95%** to **~99%**, here are the modifications:\n",
    "- Using `ReLU` as Layers 1-6 activation function\n",
    "- Remove `Softmax` from Output Layer\n",
    "\n",
    "#### After Modifications:\n",
    "<img src=https://i.imgur.com/Hwgffby.jpeg width=\"700\">\n",
    "\n",
    "*Note: If the img isn't present, viewable via link: i.imgur.com/Hwgffby.jpeg*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f64024-f366-4ae6-980b-90aa85266e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the convolutional neural network\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(400, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85cb48b-b4ad-465b-9f66-ab9889667a06",
   "metadata": {},
   "source": [
    "### Model, Loss Function, and Optimizer\n",
    "We instantiate the model, define the loss function as `CrossEntropyLoss`, and use the `Adam` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a5656e-e029-480c-af13-7127ec6eb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5(num_classes).to(device)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec66a66-d12e-4421-a872-5ac0ddd65361",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "We train the model for the specified number of epochs. During each epoch, we:\n",
    "- Forward pass: Compute the modelâ€™s predictions.\n",
    "- Compute the loss.\n",
    "- Backward pass: Compute gradients.\n",
    "- Update the model parameters using the optimizer.\n",
    "- Track the running loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "702ef574-58b3-48f5-a7f3-9788e0dbbf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.1480, Accuracy: 95.53%\n",
      "Epoch [2/20], Loss: 0.0666, Accuracy: 98.06%\n",
      "Epoch [3/20], Loss: 0.0546, Accuracy: 98.44%\n",
      "Epoch [4/20], Loss: 0.0515, Accuracy: 98.58%\n",
      "Epoch [5/20], Loss: 0.0452, Accuracy: 98.75%\n",
      "Epoch [6/20], Loss: 0.0459, Accuracy: 98.78%\n",
      "Epoch [7/20], Loss: 0.0387, Accuracy: 98.95%\n",
      "Epoch [8/20], Loss: 0.0362, Accuracy: 99.04%\n",
      "Epoch [9/20], Loss: 0.0358, Accuracy: 99.06%\n",
      "Epoch [10/20], Loss: 0.0345, Accuracy: 99.09%\n",
      "Epoch [11/20], Loss: 0.0333, Accuracy: 99.13%\n",
      "Epoch [12/20], Loss: 0.0299, Accuracy: 99.22%\n",
      "Epoch [13/20], Loss: 0.0294, Accuracy: 99.25%\n",
      "Epoch [14/20], Loss: 0.0332, Accuracy: 99.23%\n",
      "Epoch [15/20], Loss: 0.0245, Accuracy: 99.37%\n",
      "Epoch [16/20], Loss: 0.0292, Accuracy: 99.35%\n",
      "Epoch [17/20], Loss: 0.0268, Accuracy: 99.36%\n",
      "Epoch [18/20], Loss: 0.0267, Accuracy: 99.35%\n",
      "Epoch [19/20], Loss: 0.0276, Accuracy: 99.36%\n",
      "Epoch [20/20], Loss: 0.0233, Accuracy: 99.45%\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Track accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = running_loss / total_step\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "          .format(epoch+1, num_epochs, avg_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23010663-20c9-4002-8e8c-17a740010b3b",
   "metadata": {},
   "source": [
    "### Testing the Model\n",
    "We evaluate the model on the test dataset without computing gradients to save memory. We calculate the accuracy of the model on the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "865dc91b-f565-4aa2-9e12-ff0a69c7daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.85 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "  \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8319ea15-88fc-46d3-81e7-d5b05129b5a2",
   "metadata": {},
   "source": [
    "### Results\n",
    "With a modified LeNet-5 model, an accuracy of 98.85% is achieved on the MNIST test dataset.\\\n",
    "\\\n",
    "As Comparisons:\n",
    "- KNN model (k=3) accuracy: 96.33%\n",
    "- MLP model (n=128) accuracy: 96.75%\n",
    "\n",
    "### Discussion\n",
    "1. Activation Function:\n",
    "- ReLU outputs zero for negative inputs and the input itself for positive inputs, which helps maintain gradient flow during backpropagation. (Faster, more effective CNN training)\n",
    "- Tanh outputs between -1 to 1, might cause saturation for large positive/negative inputs. (Might slow down training process)\n",
    "\n",
    "2. Softmax\n",
    "- Softmax might lead to numerical instability, especially with extreme values, it might negatively impact the model's performance.\n",
    "\n",
    "### Conclusion\n",
    "The use of ReLU activation functions and reliance on `CrossEntropyLoss` for handling Softmax contribute to its superior performance, achieving a higher accuracy of 98.85%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
